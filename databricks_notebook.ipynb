{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bba72fa6-46dc-4126-bd5a-bc7ea413ddb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Notebook for UK Railway Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6848b1c3-9a10-4672-800c-35a1305265fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objective**: Analyze railway ticket transactions to uncover insights on:\n",
    "- Journey performance (on-time, delayed, cancelled)\n",
    "- Revenue patterns and pricing\n",
    "- Station performance metrics\n",
    "- Customer behavior analysis\n",
    "- **Dataset**: 31,653 railway transactions with 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11179a32-36b2-47f9-a23c-e4904a5a92b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1348ee-a25c-45e3-b60a-692ad02a2b8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# initialise spark session\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "# Read in data from Unity Catalog table\n",
    "#spark_df = spark.read.table(\"workspace.default.railway\")\n",
    "# Create Pandas DataFrame\n",
    "#df = spark_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f0b231-033c-4f9b-aefc-141cdacfcaed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defined schema for the railway dataset\n",
    "schema = StructType([\n",
    "    StructField(\"Transaction_ID\", StringType(), True),\n",
    "    StructField(\"Date_of_Purchase\", DateType(), True),\n",
    "    StructField(\"Time_of_Purchase\", StringType(), True),\n",
    "    StructField(\"Purchase_Type\", StringType(), True),\n",
    "    StructField(\"Payment_Method\", StringType(), True),\n",
    "    StructField(\"Railcard\", StringType(), True),\n",
    "    StructField(\"Ticket_Class\", StringType(), True),\n",
    "    StructField(\"Ticket_Type\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Departure_Station\", StringType(), True),\n",
    "    StructField(\"Arrival_Destination\", StringType(), True),\n",
    "    StructField(\"Date_of_Journey\", DateType(), True),\n",
    "    StructField(\"Departure_Time\", StringType(), True),\n",
    "    StructField(\"Arrival_Time\", StringType(), True),\n",
    "    StructField(\"Actual_Arrival_Time\", StringType(), True),\n",
    "    StructField(\"Journey_Status\", StringType(), True),\n",
    "    StructField(\"Reason_for_Delay\", StringType(), True),\n",
    "    StructField(\"Refund_Request\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec575f3-2b60-40e0-83d6-1f5831fe5c5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load railway data from Unity Catalog (fix DBFS error)"
    }
   },
   "outputs": [],
   "source": [
    "# Load railway data from Unity Catalog table instead of DBFS\n",
    "# This avoids DBFS_DISABLED errors and uses the managed table\n",
    "\n",
    "df_spark = spark.read.table(\"workspace.default.railway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01338ab-01cf-4ab7-bb6f-a312dc0bc6ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display basic info (fix for Unity Catalog)"
    }
   },
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "try:\n",
    "    print(f\"Dataset Shape: {df_spark.count()} rows x {len(df_spark.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(\"Could not count rows:\", e)\n",
    "    print(f\"Columns: {df_spark.columns}\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87965a8e-a663-4e31-8938-8b7c82926efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e06825-011b-499c-87b7-bed11baf9795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, count\n",
    "\n",
    "missing_values = df_spark.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_spark.columns\n",
    "])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "893f7aca-db58-4819-a457-845ad7b4b227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Key Business Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c71e51-da2a-47be-bd9d-5343dbed9a97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "# Journey Status Distribution\n",
    "journey_status = df_spark.groupBy(\"Journey Status\").agg(\n",
    "    count(\"*\").alias(\"Count\"),\n",
    "    round(count(\"*\") / df_spark.count() * 100, 2).alias(\"Percentage\")\n",
    ").orderBy(col(\"Count\").desc())\n",
    "\n",
    "journey_status.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020892d5-fc40-4ba4-99c8-161fbc05d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revenue Analysis\n",
    "revenue_metrics = df_spark.agg(\n",
    "    sum(\"Price\").alias(\"Total_Revenue\"),\n",
    "    avg(\"Price\").alias(\"Avg_Price\"),\n",
    "    max(\"Price\").alias(\"Max_Price\"),\n",
    "    min(\"Price\").alias(\"Min_Price\"),\n",
    ")\n",
    "revenue_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bb455d-2a4e-429d-8028-b537bef8f117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purchase Type Distribution\n",
    "purchase_type = df_spark.groupby(\"Purchase Type\").agg(count(\"*\")).alias(\"count\")\n",
    "display(purchase_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877a0e1-a97a-472c-926f-6e88fb4f6d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Payment Method Distribution\n",
    "payment_method = df_spark.groupby(\"Payment Method\").agg(count(\"*\")).alias(\"count\")\n",
    "display(payment_method) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
