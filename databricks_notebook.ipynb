{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba72fa6-46dc-4126-bd5a-bc7ea413ddb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Notebook for UK Railway Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6848b1c3-9a10-4672-800c-35a1305265fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objective**: Analyse railway ticket transactions to uncover insights on:\n",
    "- Journey performance (on-time, delayed, cancelled)\n",
    "- Revenue patterns and pricing\n",
    "- Station performance metrics\n",
    "- Customer behavior analysis\n",
    "- **Dataset**: 31,653 railway transactions with 18 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11179a32-36b2-47f9-a23c-e4904a5a92b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import count, when, col, sum, round\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7075af4-19a6-46be-bd5d-c03dc899171b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.  Data Loading and Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1348ee-a25c-45e3-b60a-692ad02a2b8e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# initialise spark session\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "# Read in data from Unity Catalog table\n",
    "#spark_df = spark.read.table(\"workspace.default.railway\")\n",
    "# Create Pandas DataFrame\n",
    "#df = spark_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f0b231-033c-4f9b-aefc-141cdacfcaed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defined schema for the railway dataset\n",
    "schema = StructType([\n",
    "    StructField(\"Transaction_ID\", StringType(), True),\n",
    "    StructField(\"Date_of_Purchase\", DateType(), True),\n",
    "    StructField(\"Time_of_Purchase\", StringType(), True),\n",
    "    StructField(\"Purchase_Type\", StringType(), True),\n",
    "    StructField(\"Payment_Method\", StringType(), True),\n",
    "    StructField(\"Railcard\", StringType(), True),\n",
    "    StructField(\"Ticket_Class\", StringType(), True),\n",
    "    StructField(\"Ticket_Type\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True),\n",
    "    StructField(\"Departure_Station\", StringType(), True),\n",
    "    StructField(\"Arrival_Destination\", StringType(), True),\n",
    "    StructField(\"Date_of_Journey\", DateType(), True),\n",
    "    StructField(\"Departure_Time\", StringType(), True),\n",
    "    StructField(\"Arrival_Time\", StringType(), True),\n",
    "    StructField(\"Actual_Arrival_Time\", StringType(), True),\n",
    "    StructField(\"Journey_Status\", StringType(), True),\n",
    "    StructField(\"Reason_for_Delay\", StringType(), True),\n",
    "    StructField(\"Refund_Request\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec575f3-2b60-40e0-83d6-1f5831fe5c5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load railway data from Unity Catalog (fix DBFS error)"
    }
   },
   "outputs": [],
   "source": [
    "# Load railway data from Unity Catalog table instead of DBFS\n",
    "# This avoids DBFS_DISABLED errors and uses the managed table\n",
    "\n",
    "df_spark = spark.read.table(\"workspace.default.railway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01338ab-01cf-4ab7-bb6f-a312dc0bc6ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display basic info (fix for Unity Catalog)"
    }
   },
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "try:\n",
    "    print(f\"Dataset Shape: {df_spark.count()} rows x {len(df_spark.columns)} columns\")\n",
    "except Exception as e:\n",
    "    print(\"Could not count rows:\", e)\n",
    "    print(f\"Columns: {df_spark.columns}\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87965a8e-a663-4e31-8938-8b7c82926efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e06825-011b-499c-87b7-bed11baf9795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, count\n",
    "\n",
    "missing_values = df_spark.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_spark.columns\n",
    "])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893f7aca-db58-4819-a457-845ad7b4b227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Key Business Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c71e51-da2a-47be-bd9d-5343dbed9a97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "# Journey Status Distribution\n",
    "journey_status = df_spark.groupBy(\"Journey Status\").agg(\n",
    "    count(\"*\").alias(\"Count\"),\n",
    "    round(count(\"*\") / df_spark.count() * 100, 2).alias(\"Percentage\")\n",
    ").orderBy(col(\"Count\").desc())\n",
    "\n",
    "journey_status.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020892d5-fc40-4ba4-99c8-161fbc05d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revenue Analysis\n",
    "revenue_metrics = df_spark.agg(\n",
    "    sum(\"Price\").alias(\"Total_Revenue\"),\n",
    "    avg(\"Price\").alias(\"Avg_Price\"),\n",
    "    max(\"Price\").alias(\"Max_Price\"),\n",
    "    min(\"Price\").alias(\"Min_Price\"),\n",
    ")\n",
    "revenue_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd6a1a3-9bba-48a5-b66c-d4f526cfbdff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revenue by Ticket Type\n",
    "revenue_by_type = df_spark.groupBy(\"Ticket Type\").agg(\n",
    "    sum(\"Price\").alias(\"Total_Revenue\"),\n",
    "    avg(\"Price\").alias(\"Avg_Price\"),\n",
    "    count(\"*\").alias(\"Ticket_Count\")\n",
    ").orderBy(col(\"Total_Revenue\").desc())\n",
    "\n",
    "revenue_by_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bb455d-2a4e-429d-8028-b537bef8f117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purchase Type Distribution\n",
    "purchase_type = df_spark.groupby(\"Purchase Type\").agg(count(\"*\")).alias(\"count\")\n",
    "display(purchase_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7877a0e1-a97a-472c-926f-6e88fb4f6d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Payment Method Distribution\n",
    "payment_method = df_spark.groupby(\"Payment Method\").agg(count(\"*\")).alias(\"count\")\n",
    "display(payment_method) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d0bb55-f6c6-4be6-825a-df58c8c06e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.  Delay Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb07daf8-86b7-4982-965a-01768dcd6b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delay reasons analysis\n",
    "delay_reasons = df_spark.filter(col(\"Reason for Delay\").isNotNull()) \\\n",
    "    .groupBy(\"Reason for Delay\") \\\n",
    "    .agg(count(\"*\").alias(\"Incident_Count\")) \\\n",
    "    .orderBy(col(\"Incident_Count\").desc())\n",
    "\n",
    "delay_reasons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67a7ace-3478-458c-bd65-a238abec5b65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 19"
    }
   },
   "outputs": [],
   "source": [
    "# Station delay rates (minimum 100 bookings)\n",
    "from pyspark.sql.functions import count, when, col, sum, round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "station_stats = df_spark.groupBy(\"Departure Station\").agg(\n",
    "    count(\"*\").alias(\"Total Bookings\"),\n",
    "    sum(when(col(\"Journey Status\") == \"Delayed\", 1).otherwise(0)).alias(\"Delayed Count\")\n",
    ").filter(col(\"Total Bookings\") >= 100)\n",
    "\n",
    "station_delay_rate = station_stats.withColumn(\n",
    "    \"Delay Rate\", \n",
    "    round(col(\"Delayed Count\") / col(\"Total Bookings\") * 100, 2)\n",
    ").orderBy(col(\"Delay Rate\").desc())\n",
    "\n",
    "station_delay_rate.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c33a514a-64a0-4389-a0a5-aa245b6d15f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Refund Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dade3ba-a4db-4711-bb45-a844cacff32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Refund request rate by journey status\n",
    "refund_analysis = df_spark.groupBy(\"Journey Status\").agg(\n",
    "    count(\"*\").alias(\"Total Journeys\"),\n",
    "    sum(when(col(\"Refund Request\") == \"Yes\", 1).otherwise(0)).alias(\"Refund Requests\")\n",
    ").withColumn(\n",
    "    \"Refund Rate Pct\", \n",
    "    round(col(\"Refund Requests\") / col(\"Total Journeys\") * 100, 2)\n",
    ")\n",
    "\n",
    "refund_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e3597b-fe9c-4150-999d-775b36b8b332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Customer Behavior Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac57494-5473-4e67-b71f-aca649d524bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Purchase type distribution\n",
    "purchase_behavior = df_spark.groupBy(\"Purchase Type\", \"Ticket Class\").agg(\n",
    "    count(\"*\").alias(\"Count\"),\n",
    "    avg(\"Price\").alias(\"Avg_Price\")\n",
    ").orderBy(\"Purchase Type\", \"Ticket Class\")\n",
    "\n",
    "purchase_behavior.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630f14a2-1473-46d5-9d8b-261f55a184ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Payment method preferences\n",
    "payment_stats = df_spark.groupBy(\"Payment Method\").agg(\n",
    "    count(\"*\").alias(\"Count\"),\n",
    "    round(count(\"*\") / df_spark.count() * 100, 2).alias(\"Percentage\")\n",
    ").orderBy(col(\"Count\").desc())\n",
    "\n",
    "payment_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f86325-daaa-4ce6-9e2d-9e2f0a2e87e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Top Routes Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177dd308-7716-4f66-b3e2-fcc7192ccebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  created route column and find top routes\n",
    "df_with_route = df_spark.withColumn(\n",
    "    \"Route\", \n",
    "    concat(col(\"Departure Station\"), lit(\" → \"), col(\"Arrival Destination\"))\n",
    ")\n",
    "\n",
    "top_routes = df_with_route.groupBy(\"Route\").agg(\n",
    "    count(\"*\").alias(\"Booking_Count\"),\n",
    "    avg(\"Price\").alias(\"Avg_Price\"),\n",
    "    sum(\"Price\").alias(\"Total_Revenue\")\n",
    ").orderBy(col(\"Booking_Count\").desc())\n",
    "\n",
    "top_routes.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad4d852-dadc-4761-8e24-927b57762acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Time-Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268dbe92-198b-4292-9baa-7551e24f59d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extracted hours from departure time\n",
    "df_with_hour = df_spark.withColumn(\n",
    "    \"Departure Hour\",\n",
    "    hour(col(\"Departure Time\"))\n",
    ")\n",
    "\n",
    "# Peak hours analysis\n",
    "peak_hours = df_with_hour.groupBy(\"Departure Hour\").agg(\n",
    "    count(\"*\").alias(\"Booking_Count\")\n",
    ").orderBy(\"Departure Hour\")\n",
    "peak_hours.display(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90774a0e-61e6-437f-91f0-aa3913ad4682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Visualisation (Matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6baad656-f912-48b7-96ad-fcbe6db62549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "journey_status_pd= journey_status.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd672705-48e9-4e18-acde-ae061acf2a09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 31"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "ax.pie(journey_status_pd['Count'], labels= journey_status_pd['Journey Status'], \n",
    "       autopct='%1.1f%%', colors=colors, explode=(0.05, 0.05, 0.05))\n",
    "ax.set_title('Journey Status Distribution', fontsize=14, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f4b719-466a-4b7a-91d3-8efce2bba6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f1790f-3069-4d1d-a041-2fd176eed9bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 33"
    }
   },
   "outputs": [],
   "source": [
    "#key metrics for summary\n",
    "total_revenue = df_spark.agg(sum(\"Price\")).collect()[0][0]\n",
    "total_bookings = df_spark.count()\n",
    "delay_rate = df_spark.filter(col(\"Journey Status\") == \"Delayed\").count() / total_bookings * 100\n",
    "cancellation_rate = df_spark.filter(col(\"Journey Status\") == \"Cancelled\").count() / total_bookings * 100\n",
    "refund_rate = df_spark.filter(col(\"Refund Request\") == \"Yes\").count() / total_bookings * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"KEY INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Revenue: £{total_revenue:,}\")\n",
    "print(f\"Total Bookings: {total_bookings:,}\")\n",
    "print(f\"On-Time Performance: {100 - delay_rate - cancellation_rate:.1f}%\")\n",
    "print(f\"Delay Rate: {delay_rate:.1f}%\")\n",
    "print(f\"Cancellation Rate: {cancellation_rate:.1f}%\")\n",
    "print(f\"Refund Request Rate: {refund_rate:.1f}%\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
